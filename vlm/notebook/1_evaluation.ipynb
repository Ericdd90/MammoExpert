{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ACC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def bootstrap_confidence_interval(preds_list, labels_list, mode, calc_ci=False):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        preds: [[pred_per_img] per experiment]\n",
    "        labels: [[label_per_img] per experiment]\n",
    "    \"\"\"\n",
    "    assert mode in [\"auroc\", \"auprc\", \"acc\"], f\"mode {mode} not supported!\"\n",
    "    assert isinstance(preds_list, list) and isinstance(labels_list, list)\n",
    "    if not isinstance(preds_list[0], list):\n",
    "        preds_list = [preds_list]\n",
    "        labels_list = [labels_list]\n",
    "\n",
    "    scores = []\n",
    "    recall_with_fixed_precision90 = []\n",
    "    recall_with_fixed_precision80= []\n",
    "    precision_with_fixed_recall90 = []\n",
    "    precision_with_fixed_recall80 = []\n",
    "    for preds, labels in zip(preds_list, labels_list):\n",
    "        if mode == \"auroc\":\n",
    "            score = roc_auc_score(np.array(labels), np.array(preds))\n",
    "        elif mode == \"auprc\":\n",
    "            score = average_precision_score(np.array(labels), np.array(preds))\n",
    "            precision, recall, _ = precision_recall_curve(np.array(labels), np.array(preds))\n",
    "            recall_with_fixed_precision90.append(float(np.max(recall[precision >= 0.9])))\n",
    "            recall_with_fixed_precision80.append(float(np.max(recall[precision >= 0.8])))\n",
    "            precision_with_fixed_recall90.append(float(np.max(precision[recall >= 0.9])))\n",
    "            precision_with_fixed_recall80.append(float(np.max(precision[recall >= 0.8])))\n",
    "        else:\n",
    "            score = accuracy_score(np.array(labels), (np.array(preds) > 0.5).astype(int))\n",
    "        scores.append(score)\n",
    "    score = np.mean(scores)\n",
    "    if mode == \"auprc\":\n",
    "        rec_prec90 = np.mean(recall_with_fixed_precision90)\n",
    "        rec_prec80 = np.mean(recall_with_fixed_precision80)\n",
    "        prec_rec90 = np.mean(precision_with_fixed_recall90)\n",
    "        prec_rec80 = np.mean(precision_with_fixed_recall80)\n",
    "    if not calc_ci:\n",
    "        return f\"{score:.4f}\"\n",
    "\n",
    "    bootstrap_repeat = 1000\n",
    "    bootstrap_score_list = []\n",
    "    for idx in range(bootstrap_repeat):\n",
    "        while True:\n",
    "            try:\n",
    "                sample_idx = list(\n",
    "                    np.random.choice(range(len(labels)), size=len(labels), replace=True)\n",
    "                )\n",
    "                bootstrap_scores = []\n",
    "                for preds, labels in zip(preds_list, labels_list):\n",
    "                    if mode == \"auroc\":\n",
    "                        bootstrap_scores.append(roc_auc_score(\n",
    "                            np.array(labels)[sample_idx],\n",
    "                            np.array(preds)[sample_idx]\n",
    "                        ))\n",
    "                    elif mode == \"auprc\":\n",
    "                        bootstrap_scores.append(average_precision_score(\n",
    "                            np.array(labels)[sample_idx],\n",
    "                            np.array(preds)[sample_idx]\n",
    "                        ))\n",
    "                    else:\n",
    "                        bootstrap_scores.append(accuracy_score(\n",
    "                            np.array(labels)[sample_idx],\n",
    "                            (np.array(preds) > 0.5).astype(int)[sample_idx]\n",
    "                        ))\n",
    "                bootstrap_score_list.append(np.mean(bootstrap_scores))\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "    sorted_score_list = sorted(bootstrap_score_list)\n",
    "    score_25, score_975 = sorted_score_list[25-1], sorted_score_list[975-1]\n",
    "    score_string = f\"{score:.4f} ({2*score-score_975:.4f}, {2*score-score_25:.4f})\"\n",
    "    if mode == \"auprc\":\n",
    "        score_string += f\"; R@P90 {rec_prec90:.2f}; R@P80 {rec_prec80:.2f}; \"\n",
    "        score_string += f\"P@R90 {prec_rec90:.2f}; P@R80 {prec_rec80:.2f}\"\n",
    "    return score_string\n",
    "\n",
    "def get_prob(outputs):\n",
    "    import math\n",
    "    import numpy as np\n",
    "    logprob_list = []\n",
    "    is_answer = False\n",
    "    for idx, x in enumerate(outputs[\"logprobs\"][\"content\"]):\n",
    "        if x[\"token\"] == \"<|im_end|>\":\n",
    "            break\n",
    "        logprob_list.append(x[\"logprob\"])\n",
    "    prob = math.exp(np.mean(logprob_list))\n",
    "    if \"Benign\" in outputs[\"response\"] or \"0\" in outputs[\"response\"]:\n",
    "        prob = 1 - prob\n",
    "    label = 0\n",
    "    if \"Malignant\" in outputs[\"labels\"] or \"1\" in outputs[\"labels\"]:\n",
    "        label = 1\n",
    "    return prob, label\n",
    "\n",
    "def evaluate(json_path):\n",
    "    pred = pd.read_json(\n",
    "        json_path,\n",
    "        lines=True\n",
    "    ).to_dict(orient=\"records\")\n",
    "    probs = []\n",
    "    labels = []\n",
    "    for p in pred:\n",
    "        image = p['images'][0]['path']\n",
    "        prob, label = get_prob(p)\n",
    "        probs.append(prob)\n",
    "        labels.append(label)\n",
    "    print(\"ACC\", bootstrap_confidence_interval(probs, labels, \"acc\", calc_ci=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
